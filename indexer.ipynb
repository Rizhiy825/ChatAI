{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a122581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, json\n",
    "\n",
    "# загрузка той же модели, что использовалась при индексации\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "# два индекса: по заголовкам и по телу\n",
    "idx_title = faiss.read_index(\"smartwop_title.faiss\")\n",
    "idx_body  = faiss.read_index(\"smartwop_body.faiss\")\n",
    "\n",
    "# метаданные [{id, title, body}, ...]\n",
    "meta = json.load(open(\"smartwop_docs_meta.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "def _reconstruct_batch(index: faiss.Index, ids):\n",
    "    \"\"\"Достаём исходные эмбеддинги по списку id (IndexFlatIP поддерживает reconstruct).\"\"\"\n",
    "    vecs = [index.reconstruct(int(i)) for i in ids]\n",
    "    return np.asarray(vecs, dtype=\"float32\")\n",
    "\n",
    "def search(query: str, k: int = 5, alpha: float = 0.4, fetch: int = 16):\n",
    "    \"\"\"\n",
    "    Комбинированный поиск: score = alpha * sim(title) + (1-alpha) * sim(body)\n",
    "    - k: сколько вернуть результатов\n",
    "    - alpha: вес заголовка (0..1)\n",
    "    - fetch: сколько кандидатов забираем из каждого индекса для объединения\n",
    "    \"\"\"\n",
    "    # 1) эмбеддинги запроса для title/body (E5-подобные префиксы)\n",
    "    q_title = model.encode([f\"query_title: {query}\"], normalize_embeddings=True).astype(\"float32\")\n",
    "    q_body  = model.encode([f\"query_body: {query}\"],  normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "    # 2) быстрый предварительный топ по каждому индексу\n",
    "    topn_t = min(max(fetch, k), idx_title.ntotal)\n",
    "    topn_b = min(max(fetch, k), idx_body.ntotal)\n",
    "\n",
    "    Dt, It = idx_title.search(q_title, topn_t)  # (1, topn_t)\n",
    "    Db, Ib = idx_body.search(q_body,  topn_b)   # (1, topn_b)\n",
    "\n",
    "    # 3) множество кандидатов из обоих топов\n",
    "    cand_ids = sorted(set(It[0].tolist() + Ib[0].tolist()))\n",
    "    if not cand_ids:\n",
    "        return []\n",
    "\n",
    "    # 4) точный пересчёт: достаём эмбеддинги кандидатов и считаем скоры\n",
    "    #    (векторы нормированы -> dot == cosine)\n",
    "    E_t = _reconstruct_batch(idx_title, cand_ids)   # (C, dim)\n",
    "    E_b = _reconstruct_batch(idx_body,  cand_ids)   # (C, dim)\n",
    "\n",
    "    s_title = (q_title @ E_t.T)[0]                  # (C,)\n",
    "    s_body  = (q_body  @ E_b.T)[0]                  # (C,)\n",
    "    scores  = alpha * s_title + (1.0 - alpha) * s_body\n",
    "\n",
    "    # 5) ранжируем и собираем ответ\n",
    "    order = np.argsort(-scores)[:k]\n",
    "    results = []\n",
    "    for r in order:\n",
    "        idx = int(cand_ids[r])\n",
    "        results.append({\n",
    "            \"score\": float(scores[r]),\n",
    "            **meta[idx]  # meta[idx] соответствует порядку индексации\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# пример\n",
    "print(search(\"Как изменить вид на изометрический?\", k=3, alpha=0.4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from openai import OpenAI\n",
    "from openai import BadRequestError\n",
    "\n",
    "# ====== НАСТРОЙКИ ПЕРЕКЛЮЧЕНИЯ ======\n",
    "USE_LOCAL_LM: bool = bool(os.environ.get(\"USE_LOCAL_LM\", \"1\") == \"1\")  # поставьте 1 чтобы включить локальный режим\n",
    "LOCAL_LM_BASE_URL: str = os.environ.get(\"LOCAL_LM_BASE_URL\", \"http://localhost:1234/v1\")\n",
    "LOCAL_LM_API_KEY: str = os.environ.get(\"LOCAL_LM_API_KEY\", \"lm-studio\")  # любой непустой токен\n",
    "LOCAL_LM_MODEL: str = os.environ.get(\"LOCAL_LM_MODEL\", \"qwen/qwen3-8b\")\n",
    "\n",
    "# ====== КЛИЕНТЫ ======\n",
    "# Облачный\n",
    "client_cloud = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Локальный (LM Studio OpenAI-compatible /v1/chat/completions)\n",
    "client_local = OpenAI(base_url=LOCAL_LM_BASE_URL, api_key=LOCAL_LM_API_KEY)\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You are a helpful assistant for a CAD app (SmartWOP). \"\n",
    "    \"Answer ONLY using the provided context. If the answer is not in context, say you don't know. \"\n",
    "    \"Prefer step-by-step, concise instructions. Keep tool/command names verbatim.\"\n",
    ")\n",
    "\n",
    "def build_context_blocks(hits: List[Dict], *, max_chars_per_block: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Формирует контекст без [n]-меток:\n",
    "    <TITLE>\\n<BODY>\\n\\n...\n",
    "    max_chars_per_block > 0 — мягкая обрезка каждого блока по символам (после тримминга).\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    for h in hits:\n",
    "        title = (h.get(\"title\") or \"\").strip()\n",
    "        body  = (h.get(\"body\")  or \"\").strip()\n",
    "\n",
    "        # опционально: компактная нормализация пробелов\n",
    "        title = re.sub(r\"\\s+\", \" \", title)\n",
    "        body  = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "        block = f\"{title}\\n{body}\" if title else body\n",
    "\n",
    "        if max_chars_per_block and len(block) > max_chars_per_block:\n",
    "            block = block[:max_chars_per_block].rstrip() + \"…\"\n",
    "\n",
    "        blocks.append(block)\n",
    "\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# ====== ОБЩИЙ ВСПОМОГАТЕЛЬНЫЙ ШАГ: собираем user-промпт ======\n",
    "def _build_user_prompt(user_query: str, context: str) -> str:\n",
    "    return (\n",
    "        f\"User question (keep language of the user): {user_query}\\n\\n\"\n",
    "        f\"=== CONTEXT START ===\\n{context}\\n=== CONTEXT END ===\\n\\n\"\n",
    "        f\"Rules:\\n\"\n",
    "        f\"- Answer ONLY using the information from the context above.\\n\"\n",
    "        f\"- Do NOT mention block numbers or the word 'context'.\\n\"\n",
    "        f\"- Do NOT show your reasoning, inner thoughts, or explanations of how you found the answer.\\n\"\n",
    "        f\"- Provide only the final clear instructions or factual answer.\\n\"\n",
    "        f\"- Answer in the same language as the user question.\\n\"\n",
    "        f\"- If the answer is not in context, say 'I don't know'.\\n\"\n",
    "    )\n",
    "\n",
    "# ====== ВАШ ОБЛАЧНЫЙ ПУТЬ (responses API) — без изменений логики ======\n",
    "def _create_response_safe_cloud(*, model: str, instructions: str, input_payload,\n",
    "                                max_output_tokens: int = 400, temperature: Optional[float] = None):\n",
    "    kwargs = dict(\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        input=input_payload,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        store=False,\n",
    "    )\n",
    "    if temperature is not None:\n",
    "        kwargs[\"temperature\"] = temperature\n",
    "        try:\n",
    "            return client_cloud.responses.create(**kwargs)\n",
    "        except BadRequestError as e:\n",
    "            if \"Unsupported parameter\" in str(e) and \"temperature\" in str(e):\n",
    "                kwargs.pop(\"temperature\", None)\n",
    "                return client_cloud.responses.create(**kwargs)\n",
    "            raise\n",
    "    else:\n",
    "        return client_cloud.responses.create(**kwargs)\n",
    "\n",
    "# ====== ЛОКАЛЬНЫЙ ПУТЬ (LM Studio /v1/chat/completions) ======\n",
    "def _create_response_local_chat(*, model: str, system_text: str, user_text: str,\n",
    "                                max_tokens: int = 400, temperature: Optional[float] = 0.2):\n",
    "    # LM Studio поддерживает /v1/chat/completions; ключ можно любой непустой\n",
    "    # https://lmstudio.ai/docs/app/api/endpoints/openai  (поддерживаются /v1/chat/completions, /v1/embeddings, /v1/completions)\n",
    "    resp = client_local.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_text},\n",
    "            {\"role\": \"user\",   \"content\": user_text},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False,\n",
    "    )\n",
    "    return resp\n",
    "\n",
    "def ask_llm_with_rag(user_query: str, top_k: int = 5,\n",
    "                     model: str = \"gpt-4o-mini\", temperature: Optional[float] = 0.2,\n",
    "                     alpha: float = 0.4, fetch: int = 16) -> str:\n",
    "    hits = search(user_query, k=top_k, alpha=alpha, fetch=fetch)\n",
    "    context = build_context_blocks(hits)\n",
    "    user_text = _build_user_prompt(user_query, context)\n",
    "\n",
    "    # Если ничего не нашли — сразу честно говорим\n",
    "    if not hits:\n",
    "        fallback = (\n",
    "            f\"User question (keep language of the user): {user_query}\\n\\n\"\n",
    "            f\"=== CONTEXT START ===\\n(no results)\\n=== CONTEXT END ===\\n\\n\"\n",
    "            f\"Rules:\\n\"\n",
    "            f\"- Answer ONLY using the context blocks above.\\n\"\n",
    "            f\"- If the answer is not in context, say you don't know.\\n\"\n",
    "            f\"- Answer in the same language as the user question.\\n\"\n",
    "        )\n",
    "        user_text = fallback\n",
    "    else:\n",
    "        context = build_context_blocks(hits)\n",
    "        user_text = _build_user_prompt(user_query, context)\n",
    "\n",
    "    if USE_LOCAL_LM:\n",
    "        # локальный режим: LM Studio (chat.completions)\n",
    "        resp = _create_response_local_chat(\n",
    "            model=os.environ.get(\"LOCAL_LM_MODEL\", LOCAL_LM_MODEL),\n",
    "            system_text=SYSTEM_INSTRUCTIONS,\n",
    "            user_text=user_text,\n",
    "            max_tokens=300,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    else:\n",
    "        # облачный режим (responses API)\n",
    "        input_payload = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": user_text}\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        resp = _create_response_safe_cloud(\n",
    "            model=model,\n",
    "            instructions=SYSTEM_INSTRUCTIONS,\n",
    "            input_payload=input_payload,\n",
    "            max_output_tokens=300,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.output_text\n",
    "\n",
    "# ====== Пример вызова ======\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"How to change view on isometric?\"\n",
    "    print(ask_llm_with_rag(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell: SmartWOP RAG UI (ipywidgets)\n",
    "import traceback\n",
    "from typing import List, Dict\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as w\n",
    "\n",
    "# ====== UI widgets ======\n",
    "title = w.HTML(\"<h2 style='margin:0'>SmartWOP Chat · RAG MVP</h2><p style='margin:4px 0 12px;color:#666'>Задай вопрос — получи ответ с цитированием источников [1], [2]…</p>\")\n",
    "\n",
    "q_input = w.Textarea(\n",
    "    value=\"Wie benutze ich woodWOP Komponente?\",\n",
    "    placeholder=\"Введите вопрос (DE/EN)...\",\n",
    "    description=\"Вопрос:\",\n",
    "    layout=w.Layout(width=\"100%\", height=\"80px\")\n",
    ")\n",
    "\n",
    "model_dd = w.Dropdown(\n",
    "    options=[\n",
    "        (\"gpt-4o-mini\", \"gpt-4o-mini\"),\n",
    "        (\"gpt-5-nano\", \"gpt-5-nano\"),\n",
    "    ],\n",
    "    value=\"gpt-4o-mini\",\n",
    "    description=\"Модель:\",\n",
    "    layout=w.Layout(width=\"300px\")\n",
    ")\n",
    "\n",
    "topk_slider = w.IntSlider(\n",
    "    value=5, min=1, max=10, step=1, description=\"Top-K:\",\n",
    "    continuous_update=False, readout=True, layout=w.Layout(width=\"300px\")\n",
    ")\n",
    "\n",
    "temp_slider = w.FloatSlider(\n",
    "    value=0.2, min=0.0, max=1.0, step=0.05, description=\"Temp:\",\n",
    "    continuous_update=False, readout=True, layout=w.Layout(width=\"300px\")\n",
    ")\n",
    "\n",
    "full_toggle = w.Checkbox(value=False, description=\"Показывать полный текст статьи при раскрытии\", indent=False)\n",
    "stream_toggle = w.Checkbox(value=False, description=\"(опционально) Стриминг ответа в консоль\", indent=False)\n",
    "\n",
    "ask_btn = w.Button(\n",
    "    description=\"Спросить\",\n",
    "    button_style=\"primary\",\n",
    "    icon=\"comment\",\n",
    "    layout=w.Layout(width=\"160px\", height=\"38px\")\n",
    ")\n",
    "\n",
    "status_out = w.Output(layout=w.Layout(border=\"1px solid #eee\", padding=\"6px\", max_height=\"80px\", overflow_y=\"auto\"))\n",
    "ctx_out = w.Output(layout=w.Layout(border=\"1px solid #eee\", padding=\"8px\", max_height=\"250px\", overflow_y=\"auto\"))\n",
    "ans_out = w.Output(layout=w.Layout(border=\"1px solid #eee\", padding=\"12px\"))\n",
    "\n",
    "controls = w.HBox([model_dd, topk_slider, temp_slider])\n",
    "toggles = w.HBox([full_toggle, stream_toggle])\n",
    "go = w.HBox([ask_btn])\n",
    "\n",
    "# ====== helpers ======\n",
    "def _render_context(hits: List[Dict], full: bool = False) -> str:\n",
    "    \"\"\"Build HTML accordion with context hits.\"\"\"\n",
    "    if not hits:\n",
    "        return \"<div style='color:#999'>Контекст не найден.</div>\"\n",
    "\n",
    "    items_html = []\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        title = h.get('title', f\"Doc {i}\")\n",
    "        text = h.get('body', '')\n",
    "        # Без f-строки, экранируем переносы заранее\n",
    "        text_html = text.replace('\\n', '<br>')\n",
    "        items_html.append(\"\"\"\n",
    "        <details style='margin:8px 0;border:1px solid #eee;border-radius:8px;padding:6px;'>\n",
    "          <summary style='cursor:pointer;font-weight:600'>\n",
    "            [{}] {}<span style='color:#999;font-weight:400'> · score={:.2f}</span>\n",
    "          </summary>\n",
    "          <div style='margin-top:6px;line-height:1.45'>{}</div>\n",
    "        </details>\n",
    "        \"\"\".format(i, title, h.get('score', 0), text_html))\n",
    "    return \"\".join(items_html)\n",
    "\n",
    "def _render_answer(text: str) -> str:\n",
    "    # лёгкая типографика с поддержкой жирного текста\n",
    "    import re\n",
    "    # Заменяем **текст** на <strong>текст</strong>\n",
    "    safe = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', text)\n",
    "    # Заменяем переносы строк на <br>\n",
    "    safe = safe.replace(\"\\n\", \"<br>\")\n",
    "    return \"<div style='font-size:15px;line-height:1.55'>{}</div>\".format(safe)\n",
    "\n",
    "# ====== main handler ======\n",
    "def on_ask_clicked(_):\n",
    "    with status_out:\n",
    "        clear_output()\n",
    "        print(\"⏳ Запрашиваю контекст и ответ...\")\n",
    "\n",
    "    with ctx_out:\n",
    "        clear_output()\n",
    "    with ans_out:\n",
    "        clear_output()\n",
    "\n",
    "    user_q = q_input.value.strip()\n",
    "    if not user_q:\n",
    "        with status_out:\n",
    "            clear_output()\n",
    "            print(\"⚠️ Введите вопрос.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1) Получаем контекст заранее, чтобы отрисовать\n",
    "        #    Пытаемся вызвать ваш search(query, k=..., full=...) если он поддерживает full\n",
    "        hits = []\n",
    "        hits = search(user_q, k=topk_slider.value)\n",
    "\n",
    "        hits = hits[:3]\n",
    "        \n",
    "        with ctx_out:\n",
    "            clear_output()\n",
    "            display(HTML(\"<b>Контекст:</b>\"))\n",
    "            display(HTML(_render_context(hits, full=full_toggle.value)))\n",
    "\n",
    "        # 2) Ответ модели (ваша функция ask_llm_with_rag)\n",
    "        #    Важно: пробрасываем top_k/model/temperature — чтобы UI реально влиял\n",
    "        answer = ask_llm_with_rag(\n",
    "            user_query=user_q,\n",
    "            top_k=topk_slider.value,\n",
    "            model=model_dd.value,\n",
    "            temperature=temp_slider.value\n",
    "        )\n",
    "\n",
    "        with ans_out:\n",
    "            clear_output()\n",
    "            display(HTML(\"<b>Ответ:</b>\"))\n",
    "            display(HTML(_render_answer(answer)))\n",
    "\n",
    "        with status_out:\n",
    "            clear_output()\n",
    "            print(\"✅ Готово.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with status_out:\n",
    "            clear_output()\n",
    "            print(\"❌ Ошибка:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "# bind\n",
    "ask_btn.on_click(on_ask_clicked)\n",
    "\n",
    "# ====== layout ======\n",
    "display(\n",
    "    w.VBox([\n",
    "        title,\n",
    "        q_input,\n",
    "        controls,\n",
    "        toggles,\n",
    "        go,\n",
    "        w.HTML(\"<hr style='border:none;border-top:1px solid #eee;margin:12px 0'>\"),\n",
    "        w.HTML(\"<b>Статус:</b>\"),\n",
    "        status_out,\n",
    "        w.HTML(\"<b>Источники:</b>\"),\n",
    "        ctx_out,\n",
    "        w.HTML(\"<b>Ответ:</b>\"),\n",
    "        ans_out\n",
    "    ], layout=w.Layout(width=\"100%\"))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
